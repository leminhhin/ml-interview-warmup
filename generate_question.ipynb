{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatCohere\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV,  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"ml-interview-warmup\"\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "vectorstore = Pinecone(index, embedding_function, \"text\")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InterviewQnA(question='What is prompt engineering?', answer='Prompt engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. The effect of prompt engineering methods can vary among models, thus requiring heavy experimentation and heuristics.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "# Define your desired data structure.\n",
    "class InterviewQnA(BaseModel):\n",
    "    question: str = Field(description=\"question about a topic\")\n",
    "    answer: str = Field(description=\"answer for the question\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator('question')\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != '?':\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=InterviewQnA)\n",
    "\n",
    "template = \"\"\"Topic: {user_topic}\n",
    "Context: {context}\n",
    "\n",
    "Based on the context above, generate a question and answer pair about the topic.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"user_topic\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "model = ChatCohere()\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "user_topic = 'prompting language model'\n",
    "context = retriever.invoke(user_topic)\n",
    "\n",
    "output = chain.invoke({\"user_topic\": user_topic, \"context\": context})\n",
    "qna_pair = parser.invoke(output)\n",
    "qna_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerReview(score=4, justification=\"User's answer is a direct paraphrase of the examiner's answer, and covers the key points. It is clear and well-structured.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_answer = \"I think prompt engineering is a method for communicating with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\"\n",
    "\n",
    "template = \"\"\"Topic: {user_topic}\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Examiner's answer: {examiner_answer}\n",
    "User's answer: {user_answer}\n",
    "\n",
    "Act as an examiner and objectively rate the user's answer on a scale of 1-5 based on:\n",
    "- How accurately it answers the question based on the given context\n",
    "- How fully it covers the key points in the examiner's answer\n",
    "- How well structured, clear and concise the response is\n",
    "\n",
    "Based on the question, context, examiner's answer and user's answer above, objectively provide a score between 1-5 and a short justification explaining your rating:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Define your desired data structure.\n",
    "class AnswerReview(BaseModel):\n",
    "    score: int = Field(description=\"score for user's answer\")\n",
    "    justification: str = Field(description=\"justification explaining the score\")\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=AnswerReview)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"user_topic\", \"context\", \"question\", \"examiner_answer\", \"user_answer\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({\"user_topic\": user_topic, \"context\": context, \"question\": qna_pair.question, \"examiner_answer\": qna_pair.answer, \"user_answer\": user_answer})\n",
    "answer_review = parser.invoke(output)\n",
    "answer_review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
