{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.trychroma.com/troubleshooting#sqlite\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatCohere\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = chroma_client.get_collection(\"ml_blog\")\n",
    "db = Chroma(client=chroma_client, collection_name=\"ml_blog\", embedding_function=embedding_function)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InterviewQnA(question='What is prompting engineering?', answer='Prompt Engineering, also known as In-Context Prompting, refers to methods for communicating with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "# Define your desired data structure.\n",
    "class InterviewQnA(BaseModel):\n",
    "    question: str = Field(description=\"question about a topic\")\n",
    "    answer: str = Field(description=\"answer for the question\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator('question')\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != '?':\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=InterviewQnA)\n",
    "\n",
    "template = \"\"\"Topic: {user_topic}\n",
    "Context: {context}\n",
    "\n",
    "Based on the context above, generate a question and answer pair about the topic.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"user_topic\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "model = ChatCohere()\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "user_topic = 'prompting language model'\n",
    "context = retriever.invoke(user_topic)\n",
    "\n",
    "output = chain.invoke({\"user_topic\": user_topic, \"context\": context})\n",
    "qna_pair = parser.invoke(output)\n",
    "qna_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerReview(score=4, justification=\"The user's answer is a paraphrase of the examiner's answer but is not as thorough. It does answer the question and gives a broad understanding of the topic but does not go into as much detail as the examiner's answer about the heavy experimentation required.\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_answer = \"I think prompt engineering is a method for communicating with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\"\n",
    "\n",
    "template = \"\"\"Topic: {user_topic}\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Examiner's answer: {examiner_answer}\n",
    "User's answer: {user_answer}\n",
    "\n",
    "Act as an examiner and objectively rate the user's answer on a scale of 1-5 based on:\n",
    "- How accurately it answers the question based on the given context\n",
    "- How fully it covers the key points in the examiner's answer\n",
    "- How well structured, clear and concise the response is\n",
    "\n",
    "Based on the question, context, examiner's answer and user's answer above, objectively provide a score between 1-5 and a short justification explaining your rating:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Define your desired data structure.\n",
    "class AnswerReview(BaseModel):\n",
    "    score: int = Field(description=\"score for user's answer\")\n",
    "    justification: str = Field(description=\"justification explaining the score\")\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=AnswerReview)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"user_topic\", \"context\", \"question\", \"examiner_answer\", \"user_answer\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({\"user_topic\": user_topic, \"context\": context, \"question\": qna_pair.question, \"examiner_answer\": qna_pair.answer, \"user_answer\": user_answer})\n",
    "answer_review = parser.invoke(output)\n",
    "answer_review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
